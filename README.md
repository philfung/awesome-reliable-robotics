# Awesome Reliable Robotics ðŸ¤–
A collection of robotics research papers demonstrating reliability and robustness in the real world.  
Common themes include:
- reward learning from human feedback and interventions
- value / progress estimation

### High Success Rate w/ Fine Tuning

| **Name** | **Date** | **Real World Success Rate** |**Project** | **Paper** | **Code** |  **Organization(s)** | **Notes** |
| --- | --- | --- | --- | --- | --- | --- | --- |
| Dyna Robotics DYNA-1 Model | 04/2025 | **99.4%** success rate over 24 hours in folding napkins with no intervention |  <a href="https://www.dyna.co/research)" target="_blank">Link</a> | | | Dyna Robotics | |
| ConRFT: A Reinforced Fine-tuning Method for VLA Models via Consistency Policy | 02/2025 | **96.3%** avg success rate across tasks <img alt="ConRFT" src="https://github.com/user-attachments/assets/15ddc8ba-59a6-448b-91db-3fefb212e8f7" /> | | <a href="https://arxiv.org/pdf/2502.05450" target="_blank">Link</a> | <a href="https://github.com/cccedric/conrft" target="_blank">Link</a> | Chinese Academy of Sciences | Online and offline fine-tuning. |
| HIL-SERL: Precise and Dexterous Robotic Manipulation via Human-in-the-Loop Reinforcement Learning | 10/2024 | **100%** success rate on a variety of tasks <img alt="HIL-SERL" src="https://github.com/user-attachments/assets/56f35ef2-e297-4fd7-a4e0-362bf441c670" />  | <a href="https://hil-serl.github.io/" target="_blank">Link</a> | <a href="https://hil-serl.github.io/static/hil-serl-paper.pdf" target="_blank">Link</a> | <a href="https://github.com/rail-berkeley/hil-serl" target="_blank">Link</a> | UC Berkeley | Online fine-tuning, human intervention allowed.  Implementation available in LeRobot. |
| RLIF: INTERACTIVE IMITATION LEARNING AS REINFORCEMENT LEARNING | 03/2024 | * **95%** success rate in cloth unfolding within 7 rounds * **100%** rate success in peg insertion within 6 rounds <img alt="RLIF" src="https://github.com/user-attachments/assets/f101b109-e813-4deb-99b1-99f2e070e007" /> | <a href="https://rlif-page.github.io/" target="_blank">Link</a> | <a href="https://arxiv.org/pdf/2311.12996" target="_blank">Link</a> | <a href="https://github.com/pd-perry/RLIF" target="_blank">Link</a> | UC Berkeley |  |

### New Tasks

| **Name** | **Date** | **Real World Success Rate** |**Project** | **Paper** | **Code** |  **Organization(s)** | **Notes** |
| --- | --- | --- | --- | --- | --- | --- | --- |
| ReWiND: Language-Guided Rewards Teach Robot Policies without New Demonstrations | 05/2025 | **50% - 100%** success rate on new tasks, **~5x** improvement over baseline &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <img alt="ReWIND" src="https://github.com/user-attachments/assets/ff5250b7-cbca-4747-aab2-7dcf257ce08b" />|  <a href="https://rewind-reward.github.io/" target="_blank">Link</a> |  <a href="https://arxiv.org/abs/2505.10911" target="_blank">Link</a> |  | USC, Amazon, KAIST | Focussed on new tasks. |
| RDT-1B: a Diffusion Foundation Model for Bimanual Manipulation | 03/2025 | **56%** _improvement_ in success rates across tasks over baseline (ACT, VLA, Octo) <img alt="RDT-1B" src="https://github.com/user-attachments/assets/aa8bbc19-f4d1-4006-aea3-65e34b30fd1b" />| <a href="https://rdt-robotics.github.io/rdt-robotics/" target="_blank">Link</a> |  <a href="https://arxiv.org/pdf/2410.07864" target="_blank">Link</a> | <a href="https://github.com/thu-ml/RoboticsDiffusionTransformer" target="_blank">Link</a> | Tsinghua | Focussed on new tasks.  Human-level inference/robot speed. |
| GVL: Vision Language Models are In-Context Value Learner | 11/2024 | **15% - 90%** success rate, **0.46** avg _improvement_ (VOC) on scale -1.0 to 1.0 over DP (diffusion policy)<img alt="GVL" src="https://github.com/user-attachments/assets/21541e78-91fd-478e-9de0-d491d3da8e44" />  | <a href="https://generative-value-learning.github.io/" target="_blank">Link</a> | <a href="https://arxiv.org/pdf/2411.04549" target="_blank">Link</a> |  | Deepmind, UPenn, Stanford | Focussed on new tasks and estimation using VLM.  |

